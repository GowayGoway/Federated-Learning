{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "import time\n",
    "\n",
    "from utils.sampling import mnist_iid, mnist_noniid, cifar_iid\n",
    "from utils.options import args_parser\n",
    "from models.Update import LocalUpdate\n",
    "from models.Nets import MLP, CNNMnist, CNNCifar\n",
    "from models.Fed import FedAvg\n",
    "from models.test import test_img\n",
    "\n",
    "def FedCS(the_idxs_users,train_time_list,upload_time_list,T_round):\n",
    "    select_idxs_users=[]   #本轮选中的用户\n",
    "    time_sum=0             #本轮时间，即是算法中的θ\n",
    "    while len(the_idxs_users)>0:\n",
    "        time_list=[max(train_time_list[i]-time_sum,0)+upload_time_list[i] for i in range(len(the_idxs_users))]\n",
    "        time_min=min(time_list)                  #最小元素\n",
    "        time_min_index=time_list.index(time_min) #最小元素的下标\n",
    "        time_sum_temp=time_sum+time_min\n",
    "        if time_sum_temp<=T_round:\n",
    "            time_sum=time_sum_temp\n",
    "            select_idxs_users.append(the_idxs_users[time_min_index])\n",
    "            del the_idxs_users[time_min_index]  #选中后删除\n",
    "            del train_time_list[time_min_index]\n",
    "            del upload_time_list[time_min_index]\n",
    "        else:\n",
    "            break\n",
    "    return select_idxs_users,time_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # parse args\n",
    "    args = args_parser()\n",
    "    args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')   #使用gpu或cpu\n",
    "    # load dataset and split users\n",
    "    if args.dataset == 'mnist':     #图片格式为28*28*1\n",
    "        #Compose函数把多个图像处理步骤放在一起\n",
    "        trans_mnist = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]) #均值和方差\n",
    "        dataset_train = datasets.MNIST('./dataset/', train=True, download=True, transform=trans_mnist) # 华为云上删掉一个点，pytorch1.4\n",
    "        dataset_test = datasets.MNIST('./dataset/', train=False, download=True, transform=trans_mnist)\n",
    "        # sample users\n",
    "        if args.iid:\n",
    "            #dict_users = mnist_iid(dataset_train, args.num_users)   #把数据集分成100份，即每份600个\n",
    "            dict_users=np.load(f'./save/dict_users_{args.num_users}_L-{args.L}-.npy',allow_pickle=True).tolist()\n",
    "        else:\n",
    "            #dict_users = mnist_noniid(dataset_train, args.num_users, args.L) #\n",
    "            dict_users=np.load(f'./save/dict_users_{args.num_users}_L-{args.L}.npy',allow_pickle=True).tolist()\n",
    "    elif args.dataset == 'cifar':   #图片格式为32*32*3\n",
    "        trans_cifar = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "        dataset_train = datasets.CIFAR10('./dataset/', train=True, download=True, transform=trans_cifar)\n",
    "        dataset_test = datasets.CIFAR10('./dataset/', train=False, download=True, transform=trans_cifar)\n",
    "        if args.iid:\n",
    "            dict_users = cifar_iid(dataset_train, args.num_users)\n",
    "        else:\n",
    "            exit('Error: only consider IID setting in CIFAR10')\n",
    "    else:\n",
    "        exit('Error: unrecognized dataset')\n",
    "    img_size = dataset_train[0][0].shape    #结果为1*28*28\n",
    "\n",
    "    # build model\n",
    "    if args.model == 'cnn' and args.dataset == 'cifar':\n",
    "        net_glob = CNNCifar(args=args).to(args.device)\n",
    "    elif args.model == 'cnn' and args.dataset == 'mnist':\n",
    "        net_glob = CNNMnist(args=args).to(args.device)\n",
    "    elif args.model == 'mlp':\n",
    "        len_in = 1\n",
    "        for x in img_size:\n",
    "            len_in *= x\n",
    "        net_glob = MLP(dim_in=len_in, dim_hidden=200, dim_out=args.num_classes).to(args.device)\n",
    "    else:\n",
    "        exit('Error: unrecognized model')\n",
    "    print(net_glob,'\\n')     #打印神经网络信息，由nn.module类提供\n",
    "\n",
    "    net_glob.load_state_dict(torch.load('./save/weight.pth'))\n",
    "\n",
    "    net_glob.train()    #启用 BatchNormalization和Dropout, 与eval函数相对\n",
    "\n",
    "    # copy weights\n",
    "    w_glob = net_glob.state_dict()  #暂存初始网络参数\n",
    "\n",
    "    # training\n",
    "    loss_train = [] #存放每进行一次FedAvg的损失\n",
    "    round_accuracy=[]\n",
    "    user_num = []\n",
    "    # cv_loss, cv_acc = [], []    #这行和下面4行参数都没用到\n",
    "    # val_loss_pre, counter = 0, 0\n",
    "    # net_best = None\n",
    "    # best_loss = None\n",
    "    # val_acc_list, net_list = [], []\n",
    "\n",
    "    lr=args.lr #学习率\n",
    "    gama=1  #信道分配\n",
    "    B=1     #信道增益\n",
    "    S=50    #模型大小\n",
    "    p=1     #传输功率\n",
    "    N0=1    #噪声功率\n",
    "    #h_sq= np.abs(np.random.exponential(1,args.num_users).tolist())  #信道增益的平方,服从指数分布\n",
    "    #all_upload_time_list=[int(S/(gama*B*np.log2(1+p*i/gama*B*N0))) for i in h_sq]   #上传时间\n",
    "    computer_level = np.random.uniform(1, 9, args.num_users).tolist()  #计算能力\n",
    "    communicate_time = [] #记录每轮通信时间\n",
    "    sigma=1/6  #训练时间修正因子\n",
    "    #rest_users=range(args.num_users)\n",
    "    T_round=1500 #每轮限制时间 设置1000/1500/2000\n",
    "    \n",
    "    acc_test=0\n",
    "    iter=0\n",
    "    acc=30\n",
    "    # time_start=time.time() #计时开始\n",
    "    #for iter in range(args.epochs):     #默认值已设为10,一个迭代进行一次FedAvg\n",
    "    while (acc_test <= acc) and (iter < 30):\n",
    "        print('Round {:3d}'.format(iter+1))\n",
    "        w_locals, loss_locals = [], []  #存放每个用户的本地模型参数和损失\n",
    "        m = max(int(args.frac * args.num_users), 1) #结果为10\n",
    "        idxs_users = np.random.choice(range(args.num_users), m, replace=False).tolist()  #在100个用户中随机选10个用户\n",
    "        h_sq= np.abs(np.random.exponential(1,args.num_users).tolist())  #信道增益的平方,服从指数分布，均值为50\n",
    "        all_upload_time_list=[int(S/(gama*B*np.log2(1+p*i/gama*B*N0))) for i in h_sq]   #上传时间\n",
    "        #rest_users=list(set(rest_users)-set(idxs_users))\n",
    "        #print('随机产生的用户为：',idxs_users)\n",
    "        sample_account=[len(dict_users[i]) for i in idxs_users]  #样本数\n",
    "        train_time_list=[int(sigma*args.local_ep*len(dict_users[i])/computer_level[i]) for i in idxs_users]      #本轮训练时间列表\n",
    "        upload_time_list=[all_upload_time_list[i] for i in idxs_users]  #本轮上传时间列表\n",
    "        # h_sq=np.abs(np.random.exponential(1,len(idxs_users)).tolist())  #信道增益的平方,服从指数分布\n",
    "        # upload_time_list=[int(S/(gama*B*np.log2(1+p*i/gama*B*N0))) for i in h_sq]   #上传时间\n",
    "        print('样本数：',sample_account)\n",
    "        print('上传时间：',upload_time_list)\n",
    "        print('训练时间：',train_time_list)\n",
    "        select_idxs_users, one_communicate_time=FedCS(idxs_users,train_time_list,upload_time_list,T_round)       #使用FedCS函数挑选用户,同时获得本轮通信时间\n",
    "        print('挑选出的用户为：',select_idxs_users,'数量为',len(select_idxs_users))\n",
    "        user_num.append(len(select_idxs_users))\n",
    "        print('本轮通信时间：',one_communicate_time)\n",
    "        for idx in select_idxs_users:\n",
    "            # if acc_test>=98.5:\n",
    "            #     lr = 0.005\n",
    "            # elif acc_test>=98:\n",
    "            #     lr = 0.01 \n",
    "            # elif acc_test>=97:\n",
    "            #     lr = 0.05\n",
    "            # else:\n",
    "            #     lr = 0.1\n",
    "            local = LocalUpdate(args=args, dataset=dataset_train, idxs=dict_users[idx], lr=lr) #idxs为一个用户的数据集，大小为600\n",
    "            w, loss = local.train(net=copy.deepcopy(net_glob).to(args.device))\n",
    "            w_locals.append(copy.deepcopy(w))\n",
    "            loss_locals.append(copy.deepcopy(loss))\n",
    "            \n",
    "        # update global weights\n",
    "\n",
    "        w_glob = FedAvg(w_locals)\n",
    "\n",
    "        # copy weight to net_glob   \n",
    "        net_glob.load_state_dict(w_glob)\n",
    "        net_glob.eval()\n",
    "        acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "        net_glob.train()\n",
    "        round_accuracy.append(acc_test)                       #获取本轮精度\n",
    "        communicate_time.append(one_communicate_time)         #获取本轮时间\n",
    "\n",
    "        # print loss\n",
    "        loss_avg = sum(loss_locals) / len(loss_locals)\n",
    "        print('Average loss：{:.3f}'.format(loss_avg))\n",
    "        print('Test accuracy：{:.2f}%'.format(acc_test),'\\n')\n",
    "        loss_train.append(loss_avg)\n",
    "        iter=iter+1\n",
    "    # time_end=time.time() #计时结束\n",
    "    # print('time cost',time_end-time_start,'s')\n",
    "\n",
    "    # plot loss curve\n",
    "    Time=time.strftime(\"%m.%d.%H.%M\", time.localtime()) #记录时间，用来画图的命名\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(range(len(loss_train)), loss_train)\n",
    "    plt.xlabel('Round')\n",
    "    plt.ylabel('Train_loss')\n",
    "    plt.savefig('./figure/{}_Train loss_{}_IID L-{}_Epochs-{}_Accuracy-{}_T round-{}.png'.format(Time,args.model,args.L,iter,acc,T_round))\n",
    "\n",
    "    # plot accuracy curve\n",
    "    plt.figure()\n",
    "    plot_x=[sum(communicate_time[:i+1]) for i in range(len(communicate_time))]  #计算横坐标\n",
    "    plt.plot(plot_x,round_accuracy)\n",
    "    plt.xlabel('Time / s')\n",
    "    plt.ylabel('Test_accurac / %')\n",
    "    plt.savefig('./figure/{}_Test accuracy_{}_IID L-{}_Epochs-{}_Accuracy-{}_T round-{}.png'.format(Time,args.model,args.L,iter,acc,T_round))\n",
    "\n",
    "    # save data\n",
    "    plot_data=[] #第一个保存训练损失，第二个保存时间，第三个保存测试精度，第四个保存用户数\n",
    "    plot_data.append(loss_train)\n",
    "    plot_data.append(plot_x)\n",
    "    plot_data.append(round_accuracy)\n",
    "    plot_data.append(user_num)\n",
    "    np.save('./figure data/{}_Figure data_{}_IID L-{}_Epochs-{}_Accuracy-{}_T round-{}.npy'.format(Time,args.model,args.L,iter,acc,T_round),plot_data)\n",
    "    print('数据保存成功')\n",
    "\n",
    "    # 华为云移动文件\n",
    "    import moxing as mox\n",
    "    mox.file.copy_parallel(\"./figure data\",\"obs://federated--learning/fedcs/figure data\")\n"
   ]
  }
 ]
}