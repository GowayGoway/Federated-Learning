{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "import time\n",
    "\n",
    "from utils.sampling import mnist_iid, mnist_noniid, cifar_iid\n",
    "from utils.options import args_parser\n",
    "from models.Update import LocalUpdate\n",
    "from models.Nets import MLP, CNNMnist, CNNCifar\n",
    "from models.Fed import FedAvg\n",
    "from models.test import test_img\n",
    "\n",
    "def online_bag(value,weight,z):\n",
    "    L = 0.1           #上界\n",
    "    U = 70           #下界\n",
    "    C = 1/(1+np.log(U/L))   #在[0,C]内都选\n",
    "    if z <= C:      # #在线背包的选取阈值\n",
    "        phi = L\n",
    "    elif z<1:\n",
    "        phi = ((U*np.e/L)**z)*(L/np.e)\n",
    "    else:\n",
    "        phi = U\n",
    "    radio = value/weight\n",
    "    if radio >= phi:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # parse args\n",
    "    args = args_parser()\n",
    "    args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')   #使用gpu或cpu\n",
    "\n",
    "\n",
    "    # load dataset and split users\n",
    "    if args.dataset == 'mnist':     #图片格式为28*28*1\n",
    "        #Compose函数把多个图像处理步骤放在一起\n",
    "        trans_mnist = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]) #均值和方差\n",
    "        dataset_train = datasets.MNIST('./dataset/', train=True, download=True, transform=trans_mnist)\n",
    "        dataset_test = datasets.MNIST('./dataset/', train=False, download=True, transform=trans_mnist)\n",
    "        # sample users\n",
    "        if args.iid:\n",
    "            #dict_users = mnist_iid(dataset_train, args.num_users)   #把数据集分成100份，即每份600个\n",
    "            dict_users=np.load(f'./save/dict_users_{args.num_users}_L-{args.L}.npy',allow_pickle=True).tolist()\n",
    "        else:\n",
    "            # dict_users = mnist_noniid(dataset_train, args.num_users)\n",
    "            dict_users=np.load(f'./save/dict_users_{args.num_users}_L-{args.L}.npy',allow_pickle=True).tolist()\n",
    "    elif args.dataset == 'cifar':   #图片格式为32*32*3\n",
    "        trans_cifar = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "        dataset_train = datasets.CIFAR10('./dataset/', train=True, download=True, transform=trans_cifar)\n",
    "        dataset_test = datasets.CIFAR10('./dataset/', train=False, download=True, transform=trans_cifar)\n",
    "        if args.iid:\n",
    "            dict_users = cifar_iid(dataset_train, args.num_users)\n",
    "        else:\n",
    "            exit('Error: only consider IID setting in CIFAR10')\n",
    "    else:\n",
    "        exit('Error: unrecognized dataset')\n",
    "    img_size = dataset_train[0][0].shape    #结果为1*28*28\n",
    "\n",
    "    # build model\n",
    "    if args.model == 'cnn' and args.dataset == 'cifar':\n",
    "        net_glob = CNNCifar(args=args).to(args.device)\n",
    "    elif args.model == 'cnn' and args.dataset == 'mnist':\n",
    "        net_glob = CNNMnist(args=args).to(args.device)\n",
    "    elif args.model == 'mlp':\n",
    "        len_in = 1\n",
    "        for x in img_size:\n",
    "            len_in *= x\n",
    "        net_glob = MLP(dim_in=len_in, dim_hidden=200, dim_out=args.num_classes).to(args.device)\n",
    "    else:\n",
    "        exit('Error: unrecognized model')\n",
    "    print(net_glob)     #打印神经网络信息，由nn.module类提供\n",
    "\n",
    "    net_glob.load_state_dict(torch.load('./save/weight.pth'))\n",
    "    net_glob.train()    #启用 BatchNormalization和Dropout, 与eval函数相对\n",
    "    # copy weights\n",
    "    w_glob = net_glob.state_dict()  #暂存初始网络参数\n",
    "\n",
    "    # training\n",
    "    loss_train = [] #存放每进行一次FedAvg的损失\n",
    "    round_accuracy=[]\n",
    "    user_num =[] # 存放每进行一次FedAvg的用户数量\n",
    "\n",
    "    lr=args.lr #学习率\n",
    "    gama=1  #信道分配\n",
    "    B=1     #信道增益\n",
    "    S=100   #模型大小\n",
    "    p=1     #传输功率\n",
    "    N0=1    #噪声功率\n",
    "    sigma=1/6\n",
    "    computer_level=np.random.uniform(1,9,args.num_users).tolist()  #计算能力\n",
    "    communicate_time=[] #记录每轮通信时间\n",
    "    T_round = 2000 #每轮限制时间 设置为1000/1500/2000\n",
    "\n",
    "    #绘图坐标\n",
    "    communicate_time = []#绘制的图像的横轴\n",
    "    acc_train_set = []#训练集上的准确率的集合\n",
    "    acc_test_set = []#测试集上的准确率的集合\n",
    "    acc_test=0\n",
    "    iter=0\n",
    "    acc=20\n",
    "\n",
    "    # time_start=time.time() #计时开始\n",
    "    while (acc_test <= acc) and (iter < 30):     #训练达到指定精度停止\n",
    "        h_sq= np.abs(np.random.exponential(1,args.num_users).tolist())  #信道增益的平方,服从指数分布\n",
    "        all_upload_time_list=[int(S/(gama*B*np.log2(1+p*i/gama*B*N0))) for i in h_sq]   #上传时间\n",
    "        print('Round {:3d}'.format(iter+1))\n",
    "        w_locals, loss_locals = [], []  #存放每个用户的本地模型参数和损失\n",
    "        w_norm_list = []            #存放每epoch训练用户的二范数\n",
    "        time_traincost = []         #存放每个用户训练时间\n",
    "        m = max(int(args.frac * args.num_users), 1) \n",
    "        idxs_users = [i for i in range(0,20)]\n",
    "        sample_account=[len(dict_users[i]) for i in idxs_users]  #样本数\n",
    "        train_time_list=[int(sigma*args.local_ep*len(dict_users[i])/computer_level[i]) for i in idxs_users]      #本轮训练时间列表\n",
    "        upload_time_list=[all_upload_time_list[i] for i in idxs_users]  #本轮上传时间列表\n",
    "        print('样本数：',sample_account)\n",
    "        print('上传时间：',upload_time_list)\n",
    "        print('训练时间：',train_time_list)\n",
    "        \n",
    "        #求二范数的过程\n",
    "        # for idx in idxs_users:\n",
    "        #     #LocalUpdate函数为一个用户的训练网络函数\n",
    "        #     local = LocalUpdate(args=args, dataset=dataset_train, idxs=dict_users[idx]) #idxs为一个用户的数据集，大小为600\n",
    "        #     ww, loss = local.train(net=copy.deepcopy(net_glob).to(args.device))\n",
    "\n",
    "        #     #求该用户二范数的变化量\n",
    "        #     w=copy.deepcopy(ww)              #某用户训练完后的权值\n",
    "        #     delta_w=copy.deepcopy(w_glob)    #初始化Δw向量\n",
    "        #     for Weight in delta_w.keys():\n",
    "        #         delta_w[Weight]=w[Weight]-w_glob[Weight] #该用户训练完后的权值和全局模型权值做差\n",
    "        #     w_norm=0\n",
    "        #     for w_name,w_par in delta_w.items(): \n",
    "        #         w_norm=w_norm+(np.linalg.norm(w_par.cpu().numpy()))**2 #依次求二范数平方再求和\n",
    "        #     w_norm_list.append(w_norm)\n",
    "        #     # print(len(dict_users[idx]),',用户',idx,':',(w_norm)**0.5)     #二范数值\n",
    "\n",
    "        #     w_locals.append(w)\n",
    "        #     loss_locals.append(copy.deepcopy(loss))\n",
    "        \n",
    "        # #计算得到各用户的容量（△Ttrain+Tupdate）\n",
    "        weight_time = [0 for _ in range(20)]       #存放用户的背包容量(深拷贝)\n",
    "        t_temp = np.array(train_time_list)          #按训练时间排序\n",
    "        index_t = np.argsort(t_temp)\n",
    "\n",
    "        # #weight_time:物品重量   w_norm_list:对应的物品价值\n",
    "        # w_norm_list = [i*1000 for i in w_norm_list]\n",
    "        # w_norm_list = list(map(int,w_norm_list))\n",
    "\n",
    "        #使用在线背包挑用户\n",
    "        weight_time[index_t[0]] = train_time_list[index_t[0]] + upload_time_list[index_t[0]]\n",
    "        for i in range(1,m):\n",
    "            weight_time[index_t[i]] = train_time_list[index_t[i]]-train_time_list[index_t[0]] + upload_time_list[index_t[i]]    \n",
    "        select_client = []\n",
    "        bag_capacity = T_round             #背包总容量\n",
    "        z = 0                           #已装背包重量\n",
    "        queue_time = 0\n",
    "        cur_train_time = train_time_list[index_t[0]]\n",
    "        for ind in index_t:\n",
    "            zj = z/bag_capacity         #归一化已装重量\n",
    "            #更新重量\n",
    "            if train_time_list[ind]>=z:  #当前用户训练时间大于等于已用的时间(背包容量)\n",
    "                weight_time[ind] = train_time_list[ind] - z + upload_time_list[ind] \n",
    "\n",
    "            else:                       ##当前用户训练时间小于已用的时间(背包容量)\n",
    "                weight_time[ind] = upload_time_list[ind]    #重量只有上传时间\n",
    "\n",
    "            #求二范数过程\n",
    "            local = LocalUpdate(args=args, dataset=dataset_train, idxs=dict_users[ind], lr=lr) #idxs为一个用户的数据集，大小为600\n",
    "            ww, loss = local.train(net=copy.deepcopy(net_glob).to(args.device))\n",
    "            #求该用户二范数的变化量\n",
    "            w=copy.deepcopy(ww)              #某用户训练完后的权值\n",
    "            delta_w=copy.deepcopy(w_glob)    #初始化Δw向量\n",
    "            for Weight in delta_w.keys():\n",
    "                delta_w[Weight]=w[Weight]-w_glob[Weight] #该用户训练完后的权值和全局模型权值做差\n",
    "            w_norm=0\n",
    "            for w_name,w_par in delta_w.items(): \n",
    "                w_norm=w_norm+(np.linalg.norm(w_par.cpu().numpy()))**2 #依次求二范数平方再求和\n",
    "            w_norm = int(w_norm*1000)\n",
    "            if train_time_list[ind] > bag_capacity:\n",
    "                break\n",
    "            #在线背包选取过程\n",
    "            if online_bag(w_norm,weight_time[ind],zj):       #判断是否选\n",
    "                if train_time_list[ind] > z :       #目前选中用户训练时间大于已用背包时间--不用排队\n",
    "                    temp = z\n",
    "                    z = train_time_list[ind]+upload_time_list[ind] #更新已用背包容量\n",
    "                    if z<=bag_capacity:     #背包塞得下\n",
    "                        select_client.append(ind)\n",
    "                        cur_train_time = train_time_list[ind]       #更新最新的最大用户训练时间\n",
    "                        w_norm_list.append(w_norm)\n",
    "                        w_locals.append(w)\n",
    "                        loss_locals.append(copy.deepcopy(loss))\n",
    "                    else:               #背包塞不下\n",
    "                        z = train_time_list[ind]            #已用背包容量回退\n",
    "                else:                   #目前选中用户训练时间小于已用背包时间--排队\n",
    "                    temp = z\n",
    "                    z = temp+upload_time_list[ind] #更新已用背包容量\n",
    "                    if z<=bag_capacity:     #背包塞得下\n",
    "                        select_client.append(ind)\n",
    "                        cur_train_time = train_time_list[ind]       #更新最新的最大用户训练时间\n",
    "                        w_norm_list.append(w_norm)\n",
    "                        w_locals.append(w)\n",
    "                        loss_locals.append(copy.deepcopy(loss))\n",
    "                    else:               #背包塞不下\n",
    "                        z = temp            #已用背包容量回退\n",
    "            else:\n",
    "                if train_time_list[ind]>z:  #用户训练时间大于已装背包容量\n",
    "                    if train_time_list[ind]<=bag_capacity:\n",
    "                        z = train_time_list[ind]\n",
    "                        continue\n",
    "                    else:           #该用户训练时间超过背包容量\n",
    "                        break\n",
    "                else:\n",
    "                    continue\n",
    "        \n",
    "        print('挑选的用户为：',select_client,'数量为：',len(select_client))\n",
    "        user_num.append(len(select_client))\n",
    "        print('本轮所耗费时间:%d'%(z))\n",
    "\n",
    "        try:\n",
    "            # update global weights\n",
    "            w_glob = FedAvg(w_locals)\n",
    "        except:\n",
    "            print('本轮未选中用户')\n",
    "\n",
    "        # copy weight to net_glob   \n",
    "        net_glob.load_state_dict(w_glob)\n",
    "\n",
    "        net_glob.eval()\n",
    "        acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "        net_glob.train()\n",
    "        round_accuracy.append(acc_test)                       #获取本轮精度\n",
    "\n",
    "        # print loss\n",
    "        try:\n",
    "            loss_avg = sum(loss_locals) / len(loss_locals)\n",
    "        except:\n",
    "            loss_avg = loss_avg\n",
    "        print('Average loss：{:.3f}'.format(loss_avg))\n",
    "        print('Test accuracy：{:.2f}%'.format(acc_test),'\\n')\n",
    "        loss_train.append(loss_avg)\n",
    " \n",
    "        communicate_time.append(z)\n",
    "        iter = iter + 1\n",
    "\n",
    "    # plot loss curve\n",
    "    Time=time.strftime(\"%m.%d.%H.%M\", time.localtime()) #记录时间，用来画图的命名\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(range(len(loss_train)), loss_train)\n",
    "    plt.xlabel('Round')\n",
    "    plt.ylabel('Train_loss')\n",
    "    plt.savefig('./figure/{}_Train loss_{}_IID L-{}_Epochs-{}_Accuracy-{}_T round-{}.png'.format(Time,args.model,args.L,iter,acc,T_round))\n",
    "\n",
    "    # plot accuracy curve\n",
    "    plt.figure()\n",
    "    plot_x=[sum(communicate_time[:i+1]) for i in range(len(communicate_time))]  #计算横坐标\n",
    "    plt.plot(plot_x,round_accuracy)\n",
    "    plt.xlabel('Time / s')\n",
    "    plt.ylabel('Test_accurac / %')\n",
    "    plt.savefig('./figure/{}_Test accuracy_{}_IID L-{}_Epochs-{}_Accuracy-{}_T round-{}.png'.format(Time,args.model,args.L,iter,acc,T_round))\n",
    "\n",
    "    # save data\n",
    "    plot_data=[] #第一个保存训练损失，第二个保存时间，第三个保存测试精度\n",
    "    plot_data.append(loss_train)\n",
    "    plot_data.append(plot_x)\n",
    "    plot_data.append(round_accuracy)\n",
    "    plot_data.append(user_num)\n",
    "    np.save('./figure data/{}_Figure data_{}_IID L-{}_Epochs-{}_Accuracy-{}_T round-{}.npy'.format(Time,args.model,args.L,iter,acc,T_round),plot_data)\n",
    "    print('数据保存成功')\n",
    "\n",
    "    # 华为云移动文件\n",
    "    import moxing as mox\n",
    "    mox.file.copy_parallel(\"./figure data\",\"obs://federated--learning/online/figure data\")"
   ]
  }
 ]
}